{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733987588.459390 1505084 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2 Pro\n",
      "W0000 00:00:1733987588.589139 1508756 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733987588.602900 1508755 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     success, img \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 69\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindPose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     lmList \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mfindPosition(img)\n\u001b[1;32m     71\u001b[0m     cTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mposeDetector.findPose\u001b[0;34m(self, img, draw)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindPose\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, draw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     imgRGB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mpose_landmarks:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m draw:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aimath/lib/python3.12/site-packages/mediapipe/python/solutions/pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aimath/lib/python3.12/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import math\n",
    "class poseDetector():\n",
    "    def __init__(self, static_image_mode=False, model_complexity=1,\n",
    "                 smooth_landmarks=True, detectionCon=0.5, trackCon=0.5):\n",
    "        self.static_image_mode = static_image_mode\n",
    "        self.model_complexity = model_complexity\n",
    "        self.smooth_landmarks = smooth_landmarks\n",
    "        self.detectionCon = detectionCon\n",
    "        self.trackCon = trackCon\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        self.pose = self.mpPose.Pose(static_image_mode=self.static_image_mode,\n",
    "                                     model_complexity=self.model_complexity,\n",
    "                                     smooth_landmarks=self.smooth_landmarks,\n",
    "                                     min_detection_confidence=self.detectionCon,\n",
    "                                     min_tracking_confidence=self.trackCon)\n",
    "    def findPose(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)\n",
    "        if self.results.pose_landmarks:\n",
    "            if draw:\n",
    "                self.mpDraw.draw_landmarks(img, self.results.pose_landmarks,\n",
    "                                           self.mpPose.POSE_CONNECTIONS)\n",
    "        return img\n",
    "    def findPosition(self, img, draw=True):\n",
    "        self.lmList = []\n",
    "        if self.results.pose_landmarks:\n",
    "            for id, lm in enumerate(self.results.pose_landmarks.landmark):\n",
    "                h, w, c = img.shape\n",
    "                # print(id, lm)\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                self.lmList.append([id, cx, cy])\n",
    "                if draw:\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 0), cv2.FILLED)\n",
    "        return self.lmList\n",
    "    def findAngle(self, img, p1, p2, p3, draw=True):\n",
    "        # Get the landmarks\n",
    "        x1, y1 = self.lmList[p1][1:]\n",
    "        x2, y2 = self.lmList[p2][1:]\n",
    "        x3, y3 = self.lmList[p3][1:]\n",
    "        # Calculate the Angle\n",
    "        angle = math.degrees(math.atan2(y3 - y2, x3 - x2) -\n",
    "                             math.atan2(y1 - y2, x1 - x2))\n",
    "        if angle < 0:\n",
    "            angle += 360\n",
    "        # print(angle)\n",
    "        # Draw\n",
    "        if draw:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (255, 255, 255), 3)\n",
    "            cv2.line(img, (x3, y3), (x2, y2), (255, 255, 255), 3)\n",
    "            cv2.circle(img, (x1, y1), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x1, y1), 15, (0, 0, 255), 2)\n",
    "            cv2.circle(img, (x2, y2), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x2, y2), 15, (0, 0, 255), 2)\n",
    "            cv2.circle(img, (x3, y3), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x3, y3), 15, (0, 0, 255), 2)\n",
    "            cv2.putText(img, str(int(angle)), (x2 - 50, y2 + 50),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)\n",
    "        return angle\n",
    "def main():\n",
    "    cap = cv2.VideoCapture('walking.mp4')\n",
    "    pTime = 0\n",
    "    detector = poseDetector()\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        img = detector.findPose(img)\n",
    "        lmList = detector.findPosition(img)\n",
    "        cTime = time.time()\n",
    "        fps = 1 / (cTime - pTime)\n",
    "        pTime = cTime\n",
    "        cv2.putText(img, str(int(fps)), (70, 50), cv2.FONT_HERSHEY_PLAIN, 3,\n",
    "                    (255, 0, 0), 3)\n",
    "        cv2.imshow(\"Image\", img)\n",
    "        cv2.waitKey(1)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image not found. Please check the file path.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import math\n",
    "\n",
    "class poseDetector():\n",
    "    def __init__(self, static_image_mode=False, model_complexity=1,\n",
    "                 smooth_landmarks=True, detectionCon=0.5, trackCon=0.5):\n",
    "        self.static_image_mode = static_image_mode\n",
    "        self.model_complexity = model_complexity\n",
    "        self.smooth_landmarks = smooth_landmarks\n",
    "        self.detectionCon = detectionCon\n",
    "        self.trackCon = trackCon\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        self.pose = self.mpPose.Pose(static_image_mode=self.static_image_mode,\n",
    "                                     model_complexity=self.model_complexity,\n",
    "                                     smooth_landmarks=self.smooth_landmarks,\n",
    "                                     min_detection_confidence=self.detectionCon,\n",
    "                                     min_tracking_confidence=self.trackCon)\n",
    "\n",
    "    def findPose(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)\n",
    "        if self.results.pose_landmarks:\n",
    "            if draw:\n",
    "                self.mpDraw.draw_landmarks(img, self.results.pose_landmarks,\n",
    "                                           self.mpPose.POSE_CONNECTIONS)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img, draw=True):\n",
    "        self.lmList = []\n",
    "        if self.results.pose_landmarks:\n",
    "            for id, lm in enumerate(self.results.pose_landmarks.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                self.lmList.append([id, cx, cy])\n",
    "                if draw:\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 0), cv2.FILLED)\n",
    "        return self.lmList\n",
    "\n",
    "    def findAngle(self, img, p1, p2, p3, draw=True):\n",
    "        x1, y1 = self.lmList[p1][1:]\n",
    "        x2, y2 = self.lmList[p2][1:]\n",
    "        x3, y3 = self.lmList[p3][1:]\n",
    "\n",
    "        angle = math.degrees(math.atan2(y3 - y2, x3 - x2) -\n",
    "                             math.atan2(y1 - y2, x1 - x2))\n",
    "        if angle < 0:\n",
    "            angle += 360\n",
    "\n",
    "        if draw:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (255, 255, 255), 3)\n",
    "            cv2.line(img, (x3, y3), (x2, y2), (255, 255, 255), 3)\n",
    "            cv2.circle(img, (x1, y1), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x1, y1), 15, (0, 0, 255), 2)\n",
    "            cv2.circle(img, (x2, y2), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x2, y2), 15, (0, 0, 255), 2)\n",
    "            cv2.circle(img, (x3, y3), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x3, y3), 15, (0, 0, 255), 2)\n",
    "            cv2.putText(img, str(int(angle)), (x2 - 50, y2 + 50),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)\n",
    "        return angle\n",
    "\n",
    "def main():\n",
    "    # Specify the path to the image\n",
    "    image_path = 'incorrect1.jpg'\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(\"Image not found. Please check the file path.\")\n",
    "        return\n",
    "\n",
    "    detector = poseDetector()\n",
    "    img = detector.findPose(img)\n",
    "    lmList = detector.findPosition(img, draw=False)\n",
    "    if len(lmList) != 0:\n",
    "        print(lmList[14])\n",
    "        cv2.circle(img, (lmList[14][1], lmList[14][2]), 15, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    # Display the processed image\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 13:54:27.894 python[89369:633333] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-time posture detection started. Press 'q' to exit.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_keypoints_from_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 71\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keypoints_from_frame\u001b[49m(frame)\n\u001b[1;32m     72\u001b[0m keypoints_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(keypoints, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_keypoints_from_frame' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_keypoints_from_frame(frame):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    with mp_pose.Pose(static_image_mode=False) as pose:\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_frame)\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = [(lm.x, lm.y, lm.z) for lm in results.pose_landmarks.landmark]\n",
    "            return np.array(keypoints).flatten()\n",
    "        else:\n",
    "            return np.zeros(99)\n",
    "\n",
    "class poseDetector():\n",
    "    def __init__(self, static_image_mode=False, model_complexity=1,\n",
    "                 smooth_landmarks=True, detectionCon=0.5, trackCon=0.5):\n",
    "        self.static_image_mode = static_image_mode\n",
    "        self.model_complexity = model_complexity\n",
    "        self.smooth_landmarks = smooth_landmarks\n",
    "        self.detectionCon = detectionCon\n",
    "        self.trackCon = trackCon\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        self.pose = self.mpPose.Pose(static_image_mode=self.static_image_mode,\n",
    "                                     model_complexity=self.model_complexity,\n",
    "                                     smooth_landmarks=self.smooth_landmarks,\n",
    "                                     min_detection_confidence=self.detectionCon,\n",
    "                                     min_tracking_confidence=self.trackCon)\n",
    "    def findPose(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)\n",
    "        if self.results.pose_landmarks:\n",
    "            if draw:\n",
    "                self.mpDraw.draw_landmarks(img, self.results.pose_landmarks,\n",
    "                                           self.mpPose.POSE_CONNECTIONS)\n",
    "        return img\n",
    "    def findPosition(self, img, draw=True):\n",
    "        self.lmList = []\n",
    "        if self.results.pose_landmarks:\n",
    "            for id, lm in enumerate(self.results.pose_landmarks.landmark):\n",
    "                h, w, c = img.shape\n",
    "                # print(id, lm)\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                self.lmList.append([id, cx, cy])\n",
    "                if draw:\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 0), cv2.FILLED)\n",
    "        return self.lmList\n",
    "    def findAngle(self, img, p1, p2, p3, draw=True):\n",
    "        # Get the landmarks\n",
    "        x1, y1 = self.lmList[p1][1:]\n",
    "        x2, y2 = self.lmList[p2][1:]\n",
    "        x3, y3 = self.lmList[p3][1:]\n",
    "        # Calculate the Angle\n",
    "        angle = math.degrees(math.atan2(y3 - y2, x3 - x2) -\n",
    "                             math.atan2(y1 - y2, x1 - x2))\n",
    "        if angle < 0:\n",
    "            angle += 360\n",
    "        # print(angle)\n",
    "        # Draw\n",
    "        if draw:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (255, 255, 255), 3)\n",
    "            cv2.line(img, (x3, y3), (x2, y2), (255, 255, 255), 3)\n",
    "            cv2.circle(img, (x1, y1), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x1, y1), 15, (0, 0, 255), 2)\n",
    "            cv2.circle(img, (x2, y2), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x2, y2), 15, (0, 0, 255), 2)\n",
    "            cv2.circle(img, (x3, y3), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x3, y3), 15, (0, 0, 255), 2)\n",
    "            cv2.putText(img, str(int(angle)), (x2 - 50, y2 + 50),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)\n",
    "        return angle\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_AVFOUNDATION)\n",
    "    print(\"Real-time posture detection started. Press 'q' to exit.\")\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        keypoints = extract_keypoints_from_frame(frame)\n",
    "        keypoints_tensor = torch.tensor(keypoints, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(keypoints_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            posture = \"Correct\" if predicted[0] == 1 else \"Incorrect\"\n",
    "\n",
    "        cv2.putText(frame, f\"Posture: {posture}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                    (0, 255, 0) if posture == \"Correct\" else (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Posture Detection\", frame)\n",
    "\n",
    "        pTime = 0\n",
    "        detector = poseDetector()\n",
    "\n",
    "        success, img = cap.read()\n",
    "        img = detector.findPose(img)\n",
    "        lmList = detector.findPosition(img, draw=False)\n",
    "        if len(lmList) != 0:\n",
    "            print(lmList[14])\n",
    "            cv2.circle(img, (lmList[14][1], lmList[14][2]), 15, (0, 0, 255), cv2.FILLED)\n",
    "        cTime = time.time()\n",
    "        fps = 1 / (cTime - pTime)\n",
    "        pTime = cTime\n",
    "        cv2.putText(img, str(int(fps)), (70, 50), cv2.FONT_HERSHEY_PLAIN, 3,\n",
    "                    (255, 0, 0), 3)\n",
    "        cv2.imshow(\"Image\", img)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
